# -*- coding: utf-8 -*-
"""DSAI project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hZJq562EXVF_s-1BrejgR2HloK0ynK2Q

#Aviation Safety Data Analysis
"""

# Basic Libraries
import numpy as np
import folium
import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
import matplotlib as mpl
from os import path
from PIL import Image
from wordcloud import WordCloud, STOPWORDS
sb.set() # set the default Seaborn style for graphics

import urllib.request
import pickle

#Load csv from pickle, url stream corrupts csv file.
#Change to csv before submission
#aData = pd.read_csv('AviationData.csv')
aData = pickle.load(urllib.request.urlopen("http://128.199.211.231/dsai/data.pkl"))
aData.head()

"""#Data Clean-up and Handling

### Eliminate non-useful columns in dataframe
"""

drop = ['Accident.Number', 'Registration.Number'
    , 'Schedule', 'FAR.Description', 'Air.Carrier', 'Report.Status', 'Publication.Date']
sort_data = pd.DataFrame(aData)
sort_data.drop(columns=drop, inplace=True)
sort_data.head()

"""###Remove non-US rows"""

sort_data = sort_data[sort_data.Country == 'United States']

"""### Split date into year, month, day"""

sort_data['month'], sort_data['day'], sort_data['year'] = zip(*sort_data['Event.Date'].map(lambda x: str(x).split('/')))

sort_data.head()

sort_data.info()

sort_data["year"]= pd.to_numeric(sort_data["year"])
sort_data["month"]= pd.to_numeric(sort_data["month"])
sort_data["day"]= pd.to_numeric(sort_data["day"])
sort_data.info()

"""## Sort injury severity
### Non-fatal, Fatal(..), Unavailable, Incident
### Non-fatal, Fatal, Unknown, Incident
### Fatal(7) is the number of fatal injuries which can be found in Total.Fatal.Injuries
"""

fatal_stat= []
deaths = []
for x in range(len(sort_data)):
    if str(sort_data.iloc[x]['Injury.Severity']) == 'Non-Fatal':
        fatal_stat.append("Non-Fatal")
        deaths.append(0)
    elif str(sort_data.iloc[x]['Injury.Severity']) == 'Unavailable':
        fatal_stat.append("Unknown")
        deaths.append(0)
    elif str(sort_data.iloc[x]['Injury.Severity']) == 'Incident':
        fatal_stat.append("Non-Fatal")
        deaths.append(0)
    else:
        fatal_stat.append("Fatal")
        str_d = sort_data.iloc[x]['Injury.Severity']
        str_d = str_d.replace(')', '(')
        num = str_d.split('(')
        deaths.append(int(num[1]))


sort_data['Fatal_status']= fatal_stat
sort_data['Deaths']= deaths

sort_data = sort_data[sort_data.Fatal_status != 'Unknown']

sort_data.head()

"""##Number of latitude and longitude does not match
### Reorganise to match for map
"""

map_lat=[]
map_long=[]
for x in range(len(sort_data)):
    if pd.isnull(sort_data.iloc[x]['Latitude']) == False and pd.isnull(sort_data.iloc[x]['Longitude']) == False:
        map_lat.append(sort_data.iloc[x]['Latitude'])
        map_long.append(sort_data.iloc[x]['Longitude'])
    else:
        map_lat.append(np.NaN)
        map_long.append(np.NaN)
sort_data['Map_latitude'] = map_lat
sort_data['Map_longitude'] = map_long

sort_data.info()

sort_data.fillna({'Map_latitude': 'Nan', 'Map_longitude': 'Nan', 'Total.Fatal.Injuries': 'Nan', 'Total.Serious.Injuries': 
                  'Nan', 'Total.Minor.Injuries': 'Nan', 'Total.Uninjured': 'Nan', 'Latitude': 'Nan', 'Longitude': 'Nan'
                 , 'Airport.Code': 'Nan', 'Aircraft.Category': 'Nan', 'Aircraft.Name': 'Nan'}, inplace=True)

sort_data.info()

clean_df = sort_data.dropna()
print(clean_df)

clean_df.info()

clean_df = clean_df.replace('Nan', np.nan)

clean_df[['Total.Fatal.Injuries', 'Total.Serious.Injuries', 'Total.Uninjured', 'Total.Minor.Injuries']] = clean_df[['Total.Fatal.Injuries', 'Total.Serious.Injuries', 'Total.Uninjured', 'Total.Minor.Injuries']].replace(np.nan, 0)



"""###Rename column headings"""

clean_df.rename(columns = {'Event.Id':'Event_id'}, inplace = True)
clean_df.rename(columns = {'Investigation.Type':'Investigation_type'}, inplace = True)
clean_df.rename(columns = {'Event.Date':'Event_date'}, inplace = True)
clean_df.rename(columns = {'Airport.Code':'Airport_code'}, inplace = True)
clean_df.rename(columns = {'Injury.Severity':'Injury_severity'}, inplace = True)
clean_df.rename(columns = {'Aircraft.Damage':'Aircraft_damage'}, inplace = True)
clean_df.rename(columns = {'Aircraft.Category':'Aircraft_category'}, inplace = True)
clean_df.rename(columns = {'Amateur.Built':'Amateur_built'}, inplace = True)
clean_df.rename(columns = {'Number.of.Engines':'Number_of_engines'}, inplace = True)
clean_df.rename(columns = {'Engine.Type':'Engine_type'}, inplace = True)
clean_df.rename(columns = {'Purpose.of.Flight':'Purpose_of_flight'}, inplace = True)
clean_df.rename(columns = {'Total.Fatal.Injuries':'Total_fatal_injuries'}, inplace = True)
clean_df.rename(columns = {'Total.Serious.Injuries':'Total_serious_injuries'}, inplace = True)
clean_df.rename(columns = {'Total.Minor.Injuries':'Total_minor_injuries'}, inplace = True)
clean_df.rename(columns = {'Total.Uninjured':'Total_uninjured'}, inplace = True)
clean_df.rename(columns = {'Weather.Condition':'Weather_condition'}, inplace = True)
clean_df.rename(columns = {'Broad.Phase.of.Flight':'Broad_phase_of_flight'}, inplace = True)
clean_df.rename(columns = {'Airport.Name':'Airport_name'}, inplace = True)

clean_df.head()

"""###Make count of non-fatal equal to count of fatal"""

fatal_total_df = clean_df[clean_df.Fatal_status == 'Fatal']
non_fatal_df = clean_df[clean_df.Fatal_status == 'Non-Fatal']

non_fatal_df = non_fatal_df.sample(n = len(fatal_total_df)) 
non_fatal_df.head()

clean_df = pd.concat([non_fatal_df, fatal_total_df], ignore_index=True)

"""###For plotting map."""

map_data = pd.DataFrame(clean_df[['Map_latitude', 'Map_longitude', 'Location', 'Fatal_status']])
map_data.info()

map_data =  map_data.dropna()
map_data.info()

"""###Variables with no values"""

aData.isnull().sum()

"""###Check the variables and their types"""

print(clean_df.dtypes)

clean_df.info()

"""#Basic Data Visualization

###Count of fatal accidents by year
"""

##need to clean up data so that will have distinct lines
sb.set(font_scale=1.4)
clean_df.set_index('year')['Total_fatal_injuries'].plot(figsize=(25, 10), linewidth=1.5, color='blue')
plt.xlabel("Year", labelpad=15)
plt.ylabel("Total Fatalities", labelpad=15)
plt.title("Total Fatalities by Year", y=1.02, fontsize=22);

clean_df['Total_passengers'] = clean_df['Total_serious_injuries'] + clean_df['Total_minor_injuries'] + clean_df['Total_fatal_injuries'] + clean_df['Total_uninjured']
sb.set(font_scale=1.4)
clean_df.set_index('year')['Total_passengers'].plot(figsize=(25, 10), linewidth=1.5, color='blue')
plt.xlabel("Year", labelpad=15)
plt.ylabel("Total Passengers", labelpad=15)
plt.title("Total Passengers by Year", y=1.02, fontsize=22);

"""#Uni-variate Exploratory Analysis"""

#Distribution for year
sb.catplot(y = "year", data = clean_df, kind = "count", height = 13,  orient="v")

#Distribution for Injury severity
sb.catplot(y = "Injury_severity", data = clean_df, kind = "count", height = 13)

#Distribution for Aircraft damage
sb.catplot(y = "Aircraft_damage", data = clean_df, kind = "count", height = 9)

#Distribution for Aircraft category
sb.catplot(y = "Aircraft_category", data = clean_df, kind = "count", height = 13)

#Distribution for Amateur built
sb.catplot(y = "Amateur_built", data = clean_df, kind = "count", height = 13)

#Distribution for Number of engines
sb.catplot(y = "Number_of_engines", data = clean_df, kind = "count", height = 13)

#Distribution for Engine type
sb.catplot(y = "Engine_type", data = clean_df, kind = "count", height = 13)

#Distribution for Purpose of flight
sb.catplot(y = "Purpose_of_flight", data = clean_df, kind = "count", height = 13)

"""#Bi-variate Exploratory Analysis"""

# Distribution of the two variables (Fatal Status and Aircraft Damage)
f, axes = plt.subplots(1, 1, figsize=(15, 15))
sb.heatmap(clean_df.groupby(['Fatal_status', 'Aircraft_damage']).size().unstack(), 
           linewidths = 1, annot = True, fmt='g', annot_kws = {"size": 18}, cmap = "Blues")

# Distribution of the two variables (Amateur built and Fatal Status)
f, axes = plt.subplots(1, 1, figsize=(15, 15))
sb.heatmap(clean_df.groupby(['Fatal_status', 'Amateur_built']).size().unstack(), 
           linewidths = 1, annot = True, fmt='g', annot_kws = {"size": 18}, cmap = "Blues")

# Distribution of the two variables (Number of Egnines and Fatal Status)
f, axes = plt.subplots(1, 1, figsize=(15, 15))
sb.heatmap(clean_df.groupby(['Fatal_status', 'Number_of_engines']).size().unstack(), 
           linewidths = 1, annot = True, fmt='g', annot_kws = {"size": 18}, cmap = "Blues")

# Distribution of the two variables (Engine Type and Fatal Status)
f, axes = plt.subplots(1, 1, figsize=(15, 15))
sb.heatmap(clean_df.groupby(['Engine_type', 'Fatal_status']).size().unstack(), 
           linewidths = 1, annot = True, fmt='g', annot_kws = {"size": 18}, cmap = "Blues")

# Distribution of the two variables (Purpose of Flight and Fatal Status)
f, axes = plt.subplots(1, 1, figsize=(15, 15))
sb.heatmap(clean_df.groupby(['Purpose_of_flight', 'Fatal_status']).size().unstack(), 
           linewidths = 1, annot = True, fmt='g', annot_kws = {"size": 18}, cmap = "Blues")

# Distribution of the two variables (Weather Condition and Fatal Status)
f, axes = plt.subplots(1, 1, figsize=(15, 15))
sb.heatmap(clean_df.groupby(['Weather_condition', 'Fatal_status']).size().unstack(), 
           linewidths = 1, annot = True, fmt='g', annot_kws = {"size": 18}, cmap = "Blues")

# Distribution of the two variables (Broad Phase of Flight and Fatal Status)
f, axes = plt.subplots(1, 1, figsize=(15, 15))
sb.heatmap(clean_df.groupby(['Broad_phase_of_flight', 'Fatal_status']).size().unstack(), 
           linewidths = 1, annot = True, fmt='g', annot_kws = {"size": 18}, cmap = "Blues")

"""#Classification Analysis

###Decision Tree Analysis

Some of the significant risk factors include weather conditions, phase of flight, and aircraft damage. We will investigate on these factors and use the results to give advisories for future flights.

The first factor to analyse is weather condition. We will start by setting up a simple Classification Problem.

Response Variable: Fatal_status       
Predictor = Weather.condition
"""

print("Data type : ", type(clean_df))
print("Data dims : ", clean_df.shape)
fatal = pd.DataFrame(clean_df['Fatal_status']) 
weather = pd.DataFrame(clean_df['Weather_condition'])

"""Set up the classification problem with Train and Test datasets. 
Train set with 52943 samples and Test set with 17647 samples
"""

import math
data_total = len(weather)
train_total = math.ceil(data_total*0.75)
test_total = math.floor(data_total*0.25)*-1

# Train Set : 52943 samples
weather_train = pd.DataFrame(weather[:train_total])
fatal_status_train = pd.DataFrame(fatal[:train_total])

# Test Set : 17647 samples
weather_test = pd.DataFrame(weather[test_total:])
fatal_status_test = pd.DataFrame(fatal[test_total:])

# Check the sample sizes
print("Train Set :", fatal_status_train.shape, weather_train.shape)
print("Test Set  :", fatal_status_test.shape, weather_test.shape)

type(fatal_status_train["Fatal_status"])

"""Basic statistical exploration and vizualization on the Train set"""

# Summary Statistics for Fatal Status Train
fatal_status_train["Fatal_status"].value_counts()

# Summary Statistics for Weather Train
weather_train.describe()

# Count Plot for Fatal Status Train
sb.countplot(fatal_status_train["Fatal_status"])

# Count Plot for Weather Train
sb.countplot(weather_train["Weather_condition"])

weather_train.Weather_condition.astype("category").cat.codes
cleanup_nums = {"Weather_condition": {"VMC": 2, "IMC": 1, "UNK":0}}
weather_train.replace(cleanup_nums, inplace=True)
weather_train.head()

# Import Decision Tree Classifier model from Scikit-Learn
from sklearn.tree import DecisionTreeClassifier

# Create a Decision Tree Classifier object
dectree = DecisionTreeClassifier(max_depth = 2)

# Train the Decision Tree Classifier model
dectree.fit(weather_train, fatal_status_train)

# Import export_graphviz from sklearn.tree
from sklearn.tree import export_graphviz

# Export the Decision Tree as a dot object
treedot = export_graphviz(dectree,                                      # the model
                          feature_names = weather_train.columns,          # the features 
                          out_file = None,                              # output file
                          filled = True,                                # node colors
                          rounded = True,                               # make pretty
                          special_characters = True)                    # postscript

# Render using graphviz
import graphviz
graphviz.Source(treedot)



"""#Bi-variate analysis

weather_df = pd.DataFrame(
"""

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split



"""# Multi-variate Decision tree with get_dummies"""

x = pd.DataFrame(clean_df[['Make', 
                           'Model', 
                           'Amateur_built', 
                           'Engine_type', 
                           'Weather_condition', 
                           'Broad_phase_of_flight'
                           ]])
x.info()

x = pd.get_dummies(x, prefix_sep='_', drop_first=True)
y = clean_df['Fatal_status'].copy()
y = y.astype('category')
y.head()

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.tree import export_graphviz
import graphviz

f, axes = plt.subplots(1, 1, figsize=(12, 8))
sb.countplot(y_train)

# Decision Tree using Train Data
dectree = DecisionTreeClassifier(max_depth = 4)  
dectree.fit(x_train, y_train)                   
# Predict Response corresponding to Predictors
y_train_pred = dectree.predict(x_train)
y_test_pred = dectree.predict(x_test)

# Check the Goodness of Fit (on Train Data)
print("Goodness of Fit of Model \tTrain Dataset")
print("Classification Accuracy \t:", dectree.score(x_train, y_train))
print()

# Check the Goodness of Fit (on Test Data)
print("Goodness of Fit of Model \tTest Dataset")
print("Classification Accuracy \t:", dectree.score(x_test, y_test))
print()

# Plot the Confusion Matrix for Train and Test
f, axes = plt.subplots(2, 1, figsize=(12, 24))
sb.heatmap(confusion_matrix(y_train, y_train_pred),
           annot = True, fmt=".0f", annot_kws={"size": 18}, ax = axes[0])
sb.heatmap(confusion_matrix(y_test, y_test_pred), 
           annot = True, fmt=".0f", annot_kws={"size": 18}, ax = axes[1])

treedot = export_graphviz(dectree,                                      # the model
                          feature_names = x_train.columns,              # the features 
                          out_file = None,                              # output file
                          filled = True,                                # node colors
                          rounded = True,                               # make pretty
                          special_characters = True)                    # postscript

graphviz.Source(treedot)

# Extract random 5 data from test to visualise Prediction
data_pred = x_test.sample(n = 15)
data_pred

data_pred_index = list(data_pred.index.values)

data_pred_items = clean_df.loc[ data_pred_index ,['Make', 'Model', 'Amateur_built', 'Engine_type', 'Purpose_of_flight', 'Weather_condition', 'Broad_phase_of_flight', 'Fatal_status'] ]
data_pred_items

# Extract Predictors for Prediction
x_pred = pd.DataFrame(data_pred)

# Predict Response corresponding to Predictors
y_pred = dectree.predict(x_pred)
y_pred

# Summarize the Actuals and Predictions
y_pred = pd.DataFrame(y_pred, columns = ["Pred_Stat"], index = data_pred.index)
#data_acc = pd.concat(

data_acc = pd.concat([data_pred_items, y_pred], axis = 1)

data_acc

y_prob = dectree.predict_proba(x_pred)
np.set_printoptions(precision = 3)
print(y_prob)

"""###Bi Variate Analysis"""

from sklearn import metrics
logreg = LogisticRegression()

logreg.fit(x_train, y_train)
y_pred = logreg.predict(x_test)

accuracy = metrics.accuracy_score(y_test, y_pred)
accuracy_percentage = 100 * accuracy
accuracy_percentage

"""###K-Means Clustering"""



"""#Additional Analysis

###Map of Accidents
"""

map_data.head()

##need to clean up data to remove null values and empty values, else map cannot display
##map_data = aData[['year', 'month', 'Map_latitude', 'Map_longitude', 'Aircraft.Damage', 'Fatal_status', 'Location']]
map_ = folium.Map(location =[40,-90], tiles="OpenStreetMap" , zoom_start = 2)

for i in range(0,len(map_data)):
  if map_data.iloc[i]['Fatal_status'] == "Fatal":
    folium.Marker([map_data.iloc[i]['Map_latitude'], map_data.iloc[i]['Map_longitude']], popup=str(map_data.iloc[i]['Location'])).add_to(map_)

map_

"""###Which purpose of flight has the most casualties?"""

inj_cols = ['Total_fatal_injuries', 'Total_serious_injuries', 'Total_minor_injuries', 'Total_uninjured']

inj_per_occ = clean_df.groupby('Purpose_of_flight')[inj_cols].sum()
inj_per_occ['Total_injured'] = inj_per_occ['Total_serious_injuries'] + inj_per_occ['Total_minor_injuries'] + inj_per_occ['Total_fatal_injuries']
inj_per_occ

inj_per_occ.loc[inj_per_occ.index[:22], ['Total_injured', 'Total_uninjured']].plot.barh(stacked=True);

"""###What happens to people?"""

injury_types = ['Total_fatal_injuries', 'Total_serious_injuries', 'Total_minor_injuries', 'Total_uninjured']
injuries_per_phase = clean_df[injury_types + ['Broad_phase_of_flight']].groupby('Broad_phase_of_flight').sum()
injuries_per_phase = injuries_per_phase.iloc[:10]
injuries_per_phase['Total_injuried'] = injuries_per_phase['Total_serious_injuries'] + injuries_per_phase['Total_fatal_injuries'] + injuries_per_phase['Total_minor_injuries']
injuries_per_phase['Total_serious_minor_injured'] = injuries_per_phase['Total_serious_injuries'] + injuries_per_phase['Total_minor_injuries']
injuries_per_phase

injuries_per_phase[['Total_injuried', 'Total_uninjured']].plot.barh();

injuries_per_phase[['Total_fatal_injuries', 'Total_serious_minor_injured']].plot.barh();



"""###Airport Hotspots"""

def google_authenticate():
  # Authenticate first so the Google Drive library can detect your credentials.
  from google.colab import auth
  auth.authenticate_user()

  from googleapiclient.discovery import build
  drive_service = build('drive', 'v3')
  return drive_service

drive_service = google_authenticate()

def read_file(file_id):
  """ 
  Download file from Google Drive 
  Argument: file_id
  Returns: downloaded file
  """
  
  file_id = file_id

  import io
  from googleapiclient.http import MediaIoBaseDownload

  request = drive_service.files().get_media(fileId=file_id)
  downloaded = io.BytesIO()
  downloader = MediaIoBaseDownload(downloaded, request)
  done = False
  while done is False:
    # _ is a placeholder for a progress object that we ignore.
    # (Our file is small, so we skip reporting progress.)
    _, done = downloader.next_chunk()

  downloaded.seek(0)
  return downloaded


image_file = read_file("1ORtUIjfzsEj6N-EZfsT5ODTgv-Bkm17F")
text = str(clean_df.Airport_name.tolist())
plane_mask = np.array(Image.open('image_file'))

stopwords = set(STOPWORDS)
stopwords.add('aircraft')
stopwords.add('plane')

wc = WordCloud(background_color="white", max_words=2000, mask=plane_mask,
               stopwords=stopwords)
wc.generate(text)

plt.figure(figsize=(10,10))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.title('Brief Summary', loc='Center', fontsize=14)
plt.savefig('./aircraft_wordcloud.png', dpi=50)
plt.show()